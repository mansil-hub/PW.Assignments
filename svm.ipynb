{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "01 . What is a Support Vector Machine (SVM)\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks.\n",
        "It works by finding the optimal hyperplane that best separates data points of different classes.\n",
        "SVM focuses on maximizing the margin between classes for better generalization.\n",
        "It supports different kernel functions\n",
        "\n",
        "\n",
        "\n",
        "02 . What is the difference between Hard Margin and Soft Margin SVM\n",
        "\n",
        "Hard Margin SVM strictly separates data with no misclassification, assuming data is perfectly linearly separable.\n",
        "Soft Margin SVM allows some misclassifications to handle noisy or overlapping data.\n",
        "Hard Margin can overfit if data isn't clean, while Soft Margin improves generalization.\n",
        "Soft Margin introduces a regularization parameter to\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "03 . What is the mathematical intuition behind SVM\n",
        "\n",
        "SVM aims to find the hyperplane that maximizes the **margin** between two classes.\n",
        "The margin is the distance between the hyperplane and the nearest data points (support vectors).\n",
        "Mathematically, it solves a **convex optimization** problem to minimize classification error while maximizing margin.\n",
        "This results in a robust boundary with better generalization to unseen\n",
        "\n",
        "\n",
        "\n",
        "04 . What is the role of Lagrange Multipliers in SVM\n",
        "\n",
        "Lagrange Multipliers are used in SVM to solve the constrained optimization problem of maximizing the margin.\n",
        "They allow converting the primal optimization problem into a dual form, which is easier to solve with kernels.\n",
        "Only support vectors have non-zero Lagrange multipliers, making the solution sparse.\n",
        "This approach enables efficient computation and handling of high-dimensional feature spaces.\n",
        "\n",
        "\n",
        "\n",
        "05 . What are Support Vectors in SVM\n",
        "\n",
        "Support Vectors are the data points that lie closest to the decision boundary (hyperplane) in an SVM model.\n",
        "They are the most critical elements of the training set because they directly influence the position of the hyperplane.\n",
        "Only these points have non-zero Lagrange multipliers in the optimization problem.\n",
        "Removing a support vector would change the decision boundary, unlike other points.\n",
        "\n",
        "\n",
        "\n",
        "06 . What is a Support Vector Classifier (SVC)\n",
        "\n",
        "A Support Vector Classifier (SVC) is the implementation of Support Vector Machine (SVM) for classification tasks in scikit-learn.\n",
        "\n",
        "\n",
        "\n",
        "07 . What is a Support Vector Regressor (SVR)\n",
        "\n",
        "A Support Vector Regressor (SVR) is a type of Support Vector Machine (SVM) used for regression tasks.\n",
        "It tries to fit the best line within a margin (epsilon) around the actual data points.\n",
        "SVR focuses on ignoring errors within the margin while minimizing the model complexity.\n",
        "It is useful for predicting continuous values with robust performance on noisy data.\n",
        "\n",
        "\n",
        "\n",
        "08 . What is the Kernel Trick in SVM4\n",
        "\n",
        "The Kernel Trick allows SVMs to operate in a high-dimensional feature space without explicitly transforming the data.\n",
        "It computes the inner products between the images of all pairs of data in the feature space using a kernel function.\n",
        "This enables SVMs to classify data that is not linearly separable in the original space.\n",
        "Common kernels include linear, polynomial, and radial basis function (RBF).\n",
        "\n",
        "\n",
        "09 . Compare Linear Kernel, Polynomial Kernel, and RBF Kernel\n",
        "\n",
        "The **Linear Kernel** is used when data is linearly separable and is fast to compute.\n",
        "The **Polynomial Kernel** models non-linear relationships by raising the dot product to a power, suitable for more complex data.\n",
        "The **RBF (Radial Basis Function) Kernel** maps data into infinite-dimensional space, effective for highly non-linear problems.\n",
        "Linear is best for simple data, polynomial for moderate complexity, and RBF for most real-world non-linear cases.\n",
        "\n",
        "\n",
        "\n",
        "10 . What is the effect of the C parameter in SVM\n",
        "\n",
        "The **C parameter** in SVM controls the trade-off between a smooth decision boundary and classifying training points correctly.\n",
        "A **small C** creates a wider margin with more tolerance for misclassification (softer margin).\n",
        "A **large C** tries to classify all training data correctly, leading to a narrower margin and potential overfitting.\n",
        "Thus, tuning C balances **generalization vs. accuracy** on training data.\n",
        "\n",
        "\n",
        "\n",
        "11 . What is the role of the Gamma parameter in RBF Kernel SVM\n",
        "\n",
        "The **Gamma** parameter in RBF Kernel SVM defines how far the influence of a single training example reaches.\n",
        "A **low gamma** means ‘far’—points far away have significant influence, resulting in smoother decision boundaries.\n",
        "A **high gamma** means ‘near’—only close points influence, creating more complex, tightly fit boundaries.\n",
        "Tuning gamma balances **model complexity and overfitting**.\n",
        "\n",
        "\n",
        "\n",
        "12 . What is the Naïve Bayes classifier, and why is it called \"Naïve\"\n",
        "\n",
        "\n",
        "The Naïve Bayes classifier is a probabilistic model based on Bayes' theorem used for classification tasks.\n",
        "It assumes that all features are independent of each other given the class label.\n",
        "This strong independence assumption is why it’s called \"Naïve.\"\n",
        "Despite this, it often performs well in practice, especially for text and spam classification.\n",
        "\n",
        "\n",
        "\n",
        "13 . What is Bayes’ Theorem\n",
        "\n",
        "Bayes’ Theorem describes the probability of an event based on prior knowledge of related conditions.\n",
        "It calculates the **posterior probability** by updating the prior probability with new evidence.\n",
        "Mathematically, it’s:\n",
        "$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$\n",
        "where $P(A|B)$ is the probability of A given B.\n",
        "\n",
        "\n",
        "\n",
        "14 .  Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes:\n",
        "\n",
        "Gaussian Naïve Bayes assumes continuous features follow a normal (Gaussian) distribution.\n",
        "Multinomial Naïve Bayes works with discrete count data, like word frequencies in text classification.\n",
        "Bernoulli Naïve Bayes models binary/boolean features, focusing on presence or absence of attributes.\n",
        "Each variant suits different data types for better classification accuracy.\n",
        "\n",
        "\n",
        "\n",
        "15 . When should you use Gaussian Naïve Bayes over other variants\n",
        "\n",
        "Use Gaussian Naïve Bayes when your features are continuous and approximately normally distributed.\n",
        "It’s ideal for datasets with real-valued inputs like height, weight, or temperature.\n",
        "Other variants like Multinomial or Bernoulli suit categorical or binary data instead.\n",
        "Choosing Gaussian helps model continuous data more accurately for better predictions.\n",
        "\n",
        "\n",
        "\n",
        "16 . What are the key assumptions made by Naïve Bayes\n",
        "\n",
        "Naïve Bayes assumes all features are independent given the class label.\n",
        "It presumes each feature contributes equally and independently to the outcome.\n",
        "It also assumes the data fits a particular distribution (e.g., Gaussian for continuous features).\n",
        "These simplifications make the model “naïve” but enable efficient computation.\n",
        "\n",
        "\n",
        "\n",
        "17 . What are the advantages and disadvantages of Naïve Bayes\n",
        "\n",
        "Advantages: Naïve Bayes is simple, fast, and works well with high-dimensional data.\n",
        "It performs well even with small training datasets and handles both binary and multi-class problems.\n",
        "Disadvantages: It assumes feature independence, which is often unrealistic.\n",
        "This can reduce accuracy when features are correlated.\n",
        "\n",
        "\n",
        "\n",
        "18 . Why is Naïve Bayes a good choice for text classification\n",
        "\n",
        "Naïve Bayes is fast and efficient for large text datasets.\n",
        "It handles high-dimensional data well, like word counts or frequencies.\n",
        "The assumption of feature independence works reasonably for text features.\n",
        "It performs robustly even with sparse data and limited training samples.\n",
        "\n",
        "\n",
        "\n",
        "19 . Compare SVM and Naïve Bayes for classification tasks\n",
        "\n",
        "SVM excels at finding optimal boundaries, especially with complex, high-dimensional data.\n",
        "Naïve Bayes is faster and simpler, ideal for large text or categorical datasets.\n",
        "SVM handles non-linear data well with kernels; Naïve Bayes assumes feature independence.\n",
        "SVM often achieves higher accuracy, but Naïve Bayes requires less training and works well with noisy data.\n",
        "\n",
        "\n",
        "\n",
        "20 .  How does Laplace Smoothing help in Naïve Bayes\n",
        "\n",
        "Laplace Smoothing adds a small constant (usually 1) to feature counts to avoid zero probabilities.\n",
        "It prevents the model from assigning zero likelihood to unseen features in the training data.\n",
        "This ensures the classifier can handle new or rare words without failing.\n",
        "Overall, it improves model robustness and accuracy.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iycDIBdpCcAx"
      }
    }
  ]
}